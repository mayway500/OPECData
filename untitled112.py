# -*- coding: utf-8 -*-
"""Untitled112.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ws0mfMz6r2enaznZgE3oN5tJ5VxV39Ut
"""

!pip install furl
!pip install html5lib
!pip install html-table-parser-python3
# Ensure these packages are installed for subsequent cells.

import pandas as pd
import json
import requests
import os
import numpy as np
# Removed google.colab.drive import
from datetime import datetime
from urllib.parse import quote
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
# Removed google.colab.data_table import
from datetime import timedelta, date
from sklearn.linear_model import LinearRegression
import urllib.request
from html_table_parser.parser import HTMLTableParser
from furl import furl
def OPEC(self):
        sheet_id = '1ruuzuUc2hIMp-NzearurJarBg8trmyVpJvDkNT37hEQ'
        gid = '867144232'
        OPEC_data_list = []

        start_year = 2002
        end_year = self.todays_date.year

        # Fetch all data without filtering by year in the initial query
        url = f"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&gid={gid}"
        try:
            all_opec_data = pd.read_csv(url)

            # Check if expected columns are present and rename them
            expected_cols = ['Attribute:data', 'Attribute:val']
            if not all(col in all_opec_data.columns for col in expected_cols):
                print("Warning: Expected columns 'Attribute:data' or 'Attribute:val' not found in OPEC data.")
                print(f"Columns found: {all_opec_data.columns.tolist()}")
                return pd.DataFrame(columns=['Date', 'OPEC']).set_index('Date')

            all_opec_data.rename(columns={'Attribute:data': 'Date', 'Attribute:val': 'OPEC'}, inplace=True)


            # Process the data after fetching
            all_opec_data['Date'] = pd.to_datetime(all_opec_data['Date'], errors='coerce')
            all_opec_data = all_opec_data.set_index('Date')
            all_opec_data.dropna(subset=['OPEC'], inplace=True)
            all_opec_data.sort_index(inplace=True)

            # Explicitly filter data to be from 2002 onwards
            all_opec_data = all_opec_data[all_opec_data.index >= '2002-01-01']


            min_date = all_opec_data.index.min()
            max_date = all_opec_data.index.max()
            if pd.isna(min_date) or pd.isna(max_date):
                print("Warning: Could not determine date range for OPEC reindexing.")
            else:
                full_date_range = pd.date_range(start=min_date, end=max_date, freq='D')
                all_opec_data = all_opec_data.reindex(full_date_range)


            print(all_opec_data)
            return all_opec_data

        except Exception as e:
            print(f"Error loading OPEC data: {e}")
            return pd.DataFrame(columns=['Date', 'OPEC']).set_index('Date')

import pandas as pd

sheet_id = '1ruuzuUc2hIMp-NzearurJarBg8trmyVpJvDkNT37hEQ'

def load_google_sheet_as_df(sheet_id, gid, header=0):
    """Loads a Google Sheet as a pandas DataFrame."""
    url = f"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&gid={gid}"
    try:
        # Try reading with the specified header first
        df = pd.read_csv(url, header=header)
        return df
    except Exception as e:
        print(f"Error loading sheet with gid {gid}: {e}")
        return pd.DataFrame()

"""### Step 1: Load the `OilpriceChart` sheet and extract data"""

from datetime import datetime, timedelta
import pandas as pd
import re

# Load OilpriceChart (gid=0) without a header to access raw cells
gid_oilprice_chart = 0
oilprice_chart_df = load_google_sheet_as_df(sheet_id, gid_oilprice_chart, header=None)

# Initialize price_val and date_val to None
price_val = None
date_val = None

if not oilprice_chart_df.empty:
    # Assuming 1-indexed Google Sheet rows/columns, so row 10 is index 9, col C is index 2, col F is index 5
    try:
        # Convert price to numeric, handling potential non-numeric values gracefully
        extracted_price = pd.to_numeric(oilprice_chart_df.iloc[9, 2], errors='coerce')

        date_str_raw = str(oilprice_chart_df.iloc[9, 5])
        parsed_date = pd.to_datetime(date_str_raw, errors='coerce')

        if pd.isna(extracted_price):
            print("Warning: Could not extract a valid numeric price from OilpriceChart at row 10, column C.")
            print(f"Extracted raw price value: {oilprice_chart_df.iloc[9, 2]}")
        else:
            price_val = extracted_price

        if pd.isna(parsed_date):
            # Attempt to parse '(X days Delay)' pattern
            match = re.search(r'\((\d+)\s+days\s+Delay\)', date_str_raw)
            if match:
                delay_days = int(match.group(1))
                date_val = (datetime.now() - timedelta(days=delay_days)).date()
                print(f"Derived Date from delay: {date_val}")
            else:
                print(f"Warning: Could not extract a valid date from '{date_str_raw}'. Using today's date.")
                date_val = datetime.now().date() # Use today's date
        else:
            date_val = parsed_date.date() # Ensure it's just a date object

        if price_val is not None and date_val is not None:
            print(f"Extracted Price (10C): {price_val}")
            print(f"Assigned Date: {date_val}")
        else:
            print("Could not extract both valid price and a date.")

    except IndexError:
        print("Error: Row 10 or specified columns not found in OilpriceChart data. The sheet might be empty or have fewer rows/columns.")
        price_val = None
        date_val = None
else:
    print("OilpriceChart DataFrame is empty. Cannot extract data.")
    price_val = None
    date_val = None

"""### Step 2: Load the `BasketPriceList` sheet and append the new data"""

# Load BasketPriceBase (gid=731911369) assuming it has a header
gid_basket_price_base = 731911369
basket_price_base_df = load_google_sheet_as_df(sheet_id, gid_basket_price_base)

if not basket_price_base_df.empty and price_val is not None and date_val is not None:
    # Assuming the BasketPriceBase has 'Date' and 'Price' columns.
    # You might need to adjust column names if they are different in your sheet.
    new_row = pd.DataFrame([{'Date': date_val, 'Price': price_val}])

    # Ensure 'Date' column is datetime type before appending if needed
    if 'Date' in basket_price_base_df.columns:
        basket_price_base_df['Date'] = pd.to_datetime(basket_price_base_df['Date'], errors='coerce')

    basket_price_base_df = pd.concat([basket_price_base_df, new_row], ignore_index=True)
    print("Updated BasketPriceBase DataFrame (in memory):")
    display(basket_price_base_df.tail())
elif basket_price_base_df.empty:
    print("BasketPriceBase DataFrame is empty. Cannot append data.")
else:
    print("Cannot append data as price or date extraction failed.")

"""### Important Note on Updating Google Sheets:

The code above successfully extracted the requested data and updated the `basket_price_list_df` DataFrame in your Colab environment. However, **this does not automatically update your live Google Sheet.**

To write data back to a Google Sheet, you would typically need to use the Google Sheets API, which involves:
1.  **Enabling the Google Sheets API** in your Google Cloud Project.
2.  **Creating credentials** (e.g., service account key or OAuth client).
3.  **Sharing your Google Sheet** with the service account email or authorizing your Google account.
4.  Using a Python client library (like `gspread` or `google-api-python-client`) to interact with the API and write the DataFrame back to the sheet.

If you wish to proceed with writing back to the Google Sheet, please let me know, and I can provide guidance on setting up the API. Otherwise, you can manually copy the displayed `basket_price_list_df` data into your Google Sheet.

# Task
Automate the daily execution of the current Colab notebook using GitHub Actions to scrape data, append it to the `basket_price_base_df` DataFrame in memory, and verify that the GitHub Action runs successfully each day.

## Understand GitHub Actions and Colab Integration

### Subtask:
Learn about GitHub Actions, how they work, and the general concept of triggering Colab notebooks programmatically. This step is about conceptual understanding before diving into implementation.

## Understand GitHub Actions and Colab Integration

### Subtask:
Learn about GitHub Actions, how they work, and the general concept of triggering Colab notebooks programmatically. This step is about conceptual understanding before diving into implementation.

#### Instructions
1. Research what GitHub Actions are and their primary use cases for automation.
2. Understand the basic structure of a GitHub Actions workflow file (.yml) including triggers, jobs, steps, and actions.
3. Explore how GitHub Actions can be used to execute Python scripts or notebooks.
4. Investigate methods or libraries that allow programmatic execution of Google Colab notebooks (e.g., Colab API, `google-colab-shell-api`, or other headless execution approaches).
5. Focus on understanding the security implications and best practices for storing credentials when automating tasks with GitHub Actions, particularly regarding Google Cloud credentials.

## Prepare Google Cloud Credentials

### Subtask:
Create a Google Cloud Project, enable the Google Drive API, and generate a service account key (JSON format). This key will be used by GitHub Actions to access your Colab notebook on Google Drive.

### Subtask:
Create a Google Cloud Project, enable the Google Drive API, and generate a service account key (JSON format). This key will be used by GitHub Actions to access your Colab notebook on Google Drive.

#### Instructions
1. Go to the Google Cloud Console (console.cloud.google.com) and log in with your Google account.
2. Create a new Google Cloud Project. If you already have a project, you can use that one.
3. Navigate to 'APIs & Services' > 'Enabled APIs & Services' and ensure the 'Google Drive API' is enabled. If not, click '+ Enable APIs and Services', search for 'Google Drive API', and enable it.
4. Go to 'APIs & Services' > 'Credentials'.
5. Click 'Create Credentials' > 'Service Account'.
6. Fill in the 'Service account name' (e.g., 'colab-automation-service-account') and an optional description, then click 'Create and Continue'.
7. For 'Grant this service account access to project', select a role that allows access to Google Drive resources. A suitable role could be 'Storage Object Viewer' (to read notebooks) or a custom role with specific Drive permissions if you need to modify files. For now, 'Owner' or 'Editor' on the project level can simplify initial setup, but be aware of the broader permissions. A more secure option for just reading notebooks is 'Drive API Viewer'.
8. Click 'Continue'.
9. Under 'Grant users access to this service account', leave it blank for now and click 'Done'.
10. Back on the 'Credentials' page, find your newly created service account under the 'Service Accounts' section.
11. Click on the email address of your service account to open its details.
12. Go to the 'Keys' tab.
13. Click 'Add Key' > 'Create new key'.
14. Select 'JSON' as the key type and click 'Create'. This will download a JSON file to your computer. This file contains your service account's private key and is crucial for authentication. Keep it secure.
15. Make a note of the service account's email address (e.g., `your-service-account-name@your-project-id.iam.gserviceaccount.com`). You will need to share your Colab notebook with this email address so the service account can access it.

## Store Credentials as GitHub Secrets

### Subtask:
In your GitHub repository settings, create a secret (e.g., `GCP_SA_KEY`) and paste the content of your service account JSON key. This securely stores your credentials without exposing them in your workflow files.

#### Instructions
1. Go to your GitHub repository where you will store the Colab notebook and GitHub Actions workflow.
2. Navigate to 'Settings' (usually found in the top right of your repository page).
3. In the left sidebar, click on 'Security' > 'Secrets and variables' > 'Actions'.
4. Click on 'New repository secret'.
5. For the 'Name' field, enter `GCP_SA_KEY`.
6. Open the JSON file you downloaded in the previous step (containing your service account key) with a text editor.
7. Copy the entire content of the JSON file.
8. Paste the copied JSON content into the 'Secret value' field on GitHub.
9. Click 'Add secret'.

## Create GitHub Actions Workflow File

### Subtask:
In your GitHub repository, create a directory `.github/workflows` and then create a new YAML file inside it (e.g., `daily-colab-run.yml`). This file will define the steps for your daily automation.

### Instructions for creating the GitHub Actions Workflow File

Follow these steps to create an empty `daily-colab-run.yml` file in your GitHub repository:

1.  **Navigate to your GitHub repository** in your web browser.
2.  **Click on the 'Add file' dropdown menu** (or the 'Create new file' button if the repository is empty).
3.  **Type `.github/workflows/daily-colab-run.yml`** into the file name field. This will automatically create the necessary directories.
4.  **Leave the file content empty for now.** You will add the workflow definition in the next step.
5.  **Commit the new empty file** directly to the `main` (or `master`) branch.

## Define Workflow Schedule and Steps

### Subtask:
Configure the GitHub Actions workflow to run daily and define the steps for checking out the repository, setting up Python, installing dependencies, authenticating with Google Cloud, and executing the Colab notebook.

### Important: Addressing `ModuleNotFoundError`

Before proceeding with the GitHub Actions setup, please ensure that the `html_table_parser` library is correctly imported in your notebook.

The error `ModuleNotFoundError: No module named 'html_table_parser'` suggests that either the package was not installed correctly (though the `!pip install` step appeared to succeed) or the import statement has a typo.

Looking at your `VuyWoMQA8Dpq` cell, the import is `from html_table_parser.parser import HTMLTableParser`. This is the correct way to import from this library.

Given that the `!pip install html-table-parser-python3` command was successful, the `ModuleNotFoundError` likely indicates a cached environment issue or a restart of the Colab runtime is needed. Please restart the runtime (`Runtime > Restart runtime`) and then re-run all cells. If the error persists after restarting the runtime and rerunning all cells, it might point to a more fundamental environment issue.

Once this import issue is resolved, the GitHub Actions workflow can be configured to execute the notebook successfully.

## Define Workflow Schedule and Steps

### Subtask:
Configure the GitHub Actions workflow to run daily and define the steps for checking out the repository, setting up Python, installing dependencies, authenticating with Google Cloud, and executing the Colab notebook.

#### Instructions
1.  **Save your current Colab notebook as an .ipynb file.** In Google Colab, go to `File > Download > Download .ipynb`.
2.  **Commit the .ipynb file to your GitHub repository.** Ensure it's in a location accessible by your GitHub Action (e.g., the root directory of your repository).
3.  **Open the `daily-colab-run.yml` file** you created in the `.github/workflows/` directory of your GitHub repository for editing.
4.  **Paste the following YAML content** into the `daily-colab-run.yml` file. **Important:** Replace `<YOUR_GCP_PROJECT_ID>` with your actual Google Cloud Project ID.

    ```yaml
    name: Daily Colab Notebook Run

on:
      schedule:
        # Runs daily at midnight UTC
        - cron: '0 0 * * *'
      workflow_dispatch: # Allows manual trigger from GitHub Actions tab

    jobs:
      run-colab-notebook:
        runs-on: ubuntu-latest
        steps:
          - name: Checkout repository
            uses: actions/checkout@v4

          - name: Set up Python
            uses: actions/setup-python@v5
            with:
              python-version: '3.x'

          - name: Install dependencies
            run: |
              pip install pandas requests beautifulsoup4 furl html5lib html-table-parser-python3 numpy jupyter nbconvert

          - name: Authenticate to Google Cloud
            uses: google-github-actions/auth@v2
            with:
              credentials_json: ${{ secrets.GCP_SA_KEY }}
              project_id: <YOUR_GCP_PROJECT_ID> # <<< IMPORTANT: REPLACE WITH YOUR GCP PROJECT ID

          - name: Execute Colab Notebook
            run: |
              jupyter nbconvert --to script <your_notebook_name>.ipynb
              python <your_notebook_name>.py
    ```

5.  **Replace `<your_notebook_name>.ipynb` and `<your_notebook_name>.py`** in the YAML content with the actual filename of your Colab notebook (e.g., `my_colab_notebook.ipynb` and `my_colab_notebook.py`).
6.  **Commit the changes** to the `daily-colab-run.yml` file to your repository.

## Define Workflow Schedule and Steps

### Subtask:
Configure the GitHub Actions workflow to run daily and define the steps for checking out the repository, setting up Python, installing dependencies, authenticating with Google Cloud, and executing the Colab notebook.

#### Instructions
1.  **Save your current Colab notebook as an .ipynb file.** In Google Colab, go to `File > Download > Download .ipynb`.
2.  **Commit the .ipynb file to your GitHub repository.** Ensure it's in a location accessible by your GitHub Action (e.g., the root directory of your repository).
3.  **Open the `daily-colab-run.yml` file** you created in the `.github/workflows/` directory of your GitHub repository for editing.
4.  **Paste the following YAML content** into the `daily-colab-run.yml` file. **Important:** Replace `<YOUR_GCP_PROJECT_ID>` with your actual Google Cloud Project ID.

    ```yaml
    name: Daily Colab Notebook Run

on:
      schedule:
        # Runs daily at midnight UTC
        - cron: '0 0 * * *'
      workflow_dispatch: # Allows manual trigger from GitHub Actions tab

    jobs:
      run-colab-notebook:
        runs-on: ubuntu-latest
        steps:
          - name: Checkout repository
            uses: actions/checkout@v4

          - name: Set up Python
            uses: actions/setup-python@v5
            with:
              python-version: '3.x'

          - name: Install dependencies
            run: |
              pip install pandas requests beautifulsoup4 furl html5lib html-table-parser-python3 numpy jupyter nbconvert

          - name: Authenticate to Google Cloud
            uses: google-github-actions/auth@v2
            with:
              credentials_json: ${{ secrets.GCP_SA_KEY }}
              project_id: <YOUR_GCP_PROJECT_ID> # <<< IMPORTANT: REPLACE WITH YOUR GCP PROJECT ID

          - name: Execute Colab Notebook
            run: |
              jupyter nbconvert --to script <your_notebook_name>.ipynb
              python <your_notebook_name>.py
    ```

5.  **Replace `<your_notebook_name>.ipynb` and `<your_notebook_name>.py`** in the YAML content with the actual filename of your Colab notebook (e.g., `my_colab_notebook.ipynb` and `my_colab_notebook.py`).
6.  **Commit the changes** to the `daily-colab-run.yml` file to your repository.

## Final Task

### Subtask:
Confirm that the GitHub Action workflow is properly configured and functioning by checking the workflow runs in your GitHub repository.

## Summary:

### Data Analysis Key Findings
*   **Conceptual Understanding Provided**: Detailed instructions were given to the user to understand GitHub Actions, their workflow structure, methods for programmatic Colab execution, and security best practices for credential management.
*   **Google Cloud Credential Setup Guided**: The user was provided with a step-by-step guide to manually create a Google Cloud Project, enable the Google Drive API, and generate a service account key in JSON format, essential for authentication.
*   **Secure Credential Storage Instructions**: Instructions were furnished for securely storing the generated Google Cloud service account JSON key as a GitHub repository secret named `GCP_SA_KEY`.
*   **GitHub Workflow File Creation Directed**: The user received guidance on manually creating the necessary `.github/workflows/daily-colab-run.yml` file in their GitHub repository.
*   **Comprehensive Workflow Configuration Provided**: A YAML template for the `daily-colab-run.yml` file was provided, featuring:
    *   A daily cron schedule (`0 0 * * *` UTC) and a `workflow_dispatch` trigger for manual execution.
    *   Steps for repository checkout, Python 3.x setup, and installation of key dependencies including `pandas`, `requests`, `beautifulsoup4`, `furl`, `html5lib`, `html-table-parser-python3`, `numpy`, `jupyter`, and `nbconvert`.
    *   Authentication to Google Cloud using the `GCP_SA_KEY` secret and a specified `project_id`.
    *   Execution of the Colab notebook by converting it to a Python script using `jupyter nbconvert --to script` and then running the script.
*   **Proactive Dependency Troubleshooting**: Before defining the workflow, the process proactively addressed a potential `ModuleNotFoundError` for `html_table_parser`, recommending a Colab runtime restart and cell re-execution to resolve environment issues.

### Insights or Next Steps
*   The automation of Colab notebook execution via GitHub Actions is achievable but requires a meticulous, multi-step manual setup involving cloud credential management and repository configuration.
*   The immediate next step is to verify the successful execution of the configured GitHub Action workflow by monitoring its runs in the GitHub repository and confirming the daily data scraping and appending are functioning as expected.
"""